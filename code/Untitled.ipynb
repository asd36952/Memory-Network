{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_utils import load_task, vectorize_data\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce\n",
    "import time\n",
    "import util\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "\n",
    "\n",
    "queries=util.load_queries_morp()\n",
    "train=[]\n",
    "for idx in range(len(queries)):\n",
    "    train.append([util.load_stories_with_query_using_title_more(queries[idx][0]),queries[idx][0],queries[idx][1],1])\n",
    "    \n",
    "import pickle\n",
    "for idx in range(len(train)):\n",
    "    pickle.dump(train[idx],open(\"./korean_memnet_search_data/korean_memnet_search_data_using_title_more_\"+str(idx)+\".pkl\",\"wb\"))\n",
    "\n",
    "vocab=[]\n",
    "story_max=0\n",
    "\n",
    "for stories,query,answer,flag in train:\n",
    "    story_max=max(story_max,len(stories))\n",
    "    for sent in stories:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    for word in query:\n",
    "        vocab.append(word)\n",
    "    for word in answer:\n",
    "        vocab.append(word)\n",
    "vocab=list(set(vocab))\n",
    "\n",
    "pickle.dump([vocab,story_max],open(\"./korean_memnet_search_data/vocab_story_max_using_title_more.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "story_size_list=[]\n",
    "for idx in range(231):\n",
    "    data=pickle.load(open(\"./korean_memnet_search_data/korean_memnet_search_data_using_title_more_\"+str(idx)+\".pkl\",\"rb\"))\n",
    "    story_size=len(data[0])\n",
    "    story_size_list.append(story_size)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg\n",
    "\n",
    "plt.hist(story_size_list,bins=100,cumulative=True)\n",
    "plt.title(\"story size histogram\")\n",
    "plt.xlabel(\"story size\")\n",
    "plt.ylabel(\"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "story_size_list=[]\n",
    "for idx in range(231):\n",
    "    data=pickle.load(open(\"./korean_memnet_search_data/korean_memnet_search_data_using_title_\"+str(idx)+\".pkl\",\"rb\"))\n",
    "    story_size=len(data[0])\n",
    "    story_size_list.append(story_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fdc29889dd8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg\n",
    "\n",
    "plt.hist(story_size_list,bins=100,cumulative=True)\n",
    "plt.title(\"story size histogram\")\n",
    "plt.xlabel(\"story size\")\n",
    "plt.ylabel(\"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "batch_size=1\n",
    "vocab,story_max=pickle.load(open(\"./korean_memnet_search_data/vocab_story_max.pkl\",\"rb\"))\n",
    "print(len(vocab))\n",
    "print(story_max)\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "with tf.Graph().as_default(),tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options,allow_soft_placement = True)) as sess:\n",
    "    tick=time.time()\n",
    "    a=MN(vocab,1,300,story_max,1,batch_size,sess)\n",
    "    print(time.time()-tick)\n",
    "    input()\n",
    "    for idx in range(231):\n",
    "        print(idx)\n",
    "        \n",
    "        tick=time.time()\n",
    "        data=pickle.load(open(\"./korean_memnet_search_data/korean_memnet_search_data_\"+str(idx)+\".pkl\",\"rb\"))\n",
    "        print(time.time()-tick)\n",
    "        tick=time.time()\n",
    "        train_data,valid_data=a.preprocessing_dict([data],1)\n",
    "        print(time.time()-tick)\n",
    "        print(len(train_data))\n",
    "        print(len(test_data))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "class MN():\n",
    "    def __init__(self,vocab,memory_dim,memory_max,story_max,hop,batch_size,session):\n",
    "        self.vocab=vocab\n",
    "        self.vocab_dict=dict()\n",
    "        for idx in range(len(vocab)):\n",
    "            self.vocab_dict[vocab[idx]]=idx\n",
    "        \n",
    "        \n",
    "        self.story_max=story_max\n",
    "        \n",
    "        self.hop=hop\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.memory_max=memory_max\n",
    "        self.memory_dim=memory_dim\n",
    "        #self.init_memory()\n",
    "        self.memory=self.init_memory()\n",
    "        \n",
    "        #self.initializer=tf.random_normal_initializer(stddev=0.1)\n",
    "        self.session=session\n",
    "        self.init_tf_variable()\n",
    "        \n",
    "        init_tf=tf.initialize_all_variables()\n",
    "        \n",
    "        self.session.run(init_tf)\n",
    "        \n",
    "    def return_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def init_tf_variable(self):\n",
    "        for d in ['/gpu:0', '/gpu:1','/gpu:2','/gpu:3']:\n",
    "            with tf.device(d):\n",
    "                #self.story_embedding_W=tf.Variable(self.initializer)\n",
    "                self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"story_embedding_W\")\n",
    "                #self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"story_embedding_W\")\n",
    "                #self.story_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"story_embedding_b\")\n",
    "\n",
    "                self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"query_embedding_W\")\n",
    "                #self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"query_embedding_W\")\n",
    "                #self.query_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"query_embedding_b\")\n",
    "\n",
    "                self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=1.0/math.sqrt(float(self.memory_dim))),name=\"output_W\")\n",
    "                #self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=0.1),name=\"output_W\")\n",
    "                #self.output_b=tf.Variable(tf.zeros([len(self.vocab)]),name=\"output_b\")\n",
    "\n",
    "                self.TA = tf.Variable(tf.truncated_normal([self.story_max,self.memory_dim],stddev=0.1))\n",
    "\n",
    "                self.story_batch=tf.placeholder(tf.float32,[self.batch_size,self.story_max,len(self.vocab)])\n",
    "                self.query_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "                self.answer_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "                self.memory_init_flag_batch=tf.placeholder(tf.int32,[self.batch_size])\n",
    "\n",
    "\n",
    "                self.cross_entropy_sum=tf.constant(0.0)\n",
    "                self.predict_list=tf.constant([[0.0]*len(self.vocab)])\n",
    "                self.cross_entropy_list=tf.constant([])\n",
    "                self.embedding_story=tf.add(tf.matmul(self.story_batch[0],self.story_embedding_W),self.TA) \n",
    "                for idx in range(self.batch_size):\n",
    "                    #print(tf.constant(1))\n",
    "                    #print()\n",
    "\n",
    "                    #self.memory=tf.mul(self.memory,tf.sub(tf.constant(1.0),tf.cast(self.memory_init_flag_batch[idx],dtype=tf.float32)))\n",
    "                    #self.memory_pointer=tf.mul(self.memory_pointer,tf.sub(tf.constant(1),self.memory_init_flag_batch[idx]))\n",
    "                    #self.embedding_story=tf.matmul(self.story_batch[idx],self.story_embedding_W)+self.story_embedding_b\n",
    "                    #embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)+self.query_embedding_b\n",
    "                    #self.memory=tf.cond(tf.equal(self.memory_init_flag_batch[idx],tf.constant(1)),self.init_memory,self.return_memory)\n",
    "\n",
    "                    embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)\n",
    "                    #tf.scatter_update(self.memory,self.memory_pointer,self.embedding_story)\n",
    "\n",
    "                    #self.memory=tf.concat(0,[self.memory,self.embedding_story])\n",
    "                    \"\"\"\n",
    "                    for elem in tf.split(0,self.embedding_story.get_shape()[0],self.embedding_story):\n",
    "                        #print(self.memory)\n",
    "                        self.memory=tf.concat(0,[self.memory,elem])\n",
    "                        #self.memorytf.reshape(elem,[-1])\n",
    "                        #self.memory_pointer=tf.add(self.memory_pointer,1)\n",
    "                    \"\"\"\n",
    "                    #self.memory=tf.concat(self.memory,self.embedding_story)\n",
    "                    self.u=embedding_query\n",
    "                    for i in range(self.hop):\n",
    "\n",
    "\n",
    "\n",
    "                        self.attention=tf.nn.softmax(tf.reshape(tf.reduce_sum(tf.mul(self.embedding_story,self.u),1),[1,-1]))\n",
    "                        #self.attention=tf.nn.softmax(tf.reshape(tf.reduce_sum(tf.mul(self.memory[1:],self.u),1),[1,-1]))\n",
    "                        self.attended_embedding=tf.matmul(self.attention,self.embedding_story)\n",
    "                        #self.attended_embedding=tf.matmul(self.attention,self.memory[1:])\n",
    "                        self.u=tf.add(self.u,self.attended_embedding)\n",
    "\n",
    "\n",
    "\n",
    "                        #self.predict=self.attended_embedding+self.embedding_query\n",
    "                        #self.predict=tf.matmul(self.attended_embedding+self.embedding_query,self.output_W)+self.output_b\n",
    "                    #predict=tf.matmul(self.u,self.output_W)+self.output_b\n",
    "                    predict=tf.matmul(self.u,self.output_W)\n",
    "                    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(predict, tf.cast(self.answer_batch[idx], tf.float32), name=\"cross_entropy\")\n",
    "                    self.cross_entropy_list=tf.concat(0,[self.cross_entropy_list,cross_entropy])\n",
    "                    #self.cross_entropy_sum=tf.add(self.cross_entropy_sum,tf.reduce_sum(cross_entropy))\n",
    "\n",
    "                    #self.predict_list=tf.cond(tf.equal(self.predict_list,tf.constant([[]])),\n",
    "                    #self.predict_list=tf.cond(tf.constant(self.predict_list==tf.constant([[]])),\n",
    "                    #                          lambda:tf.concat(1,[self.predict_list,tf.nn.softmax(predict)]),lambda:tf.concat(0,[self.predict_list,tf.nn.softmax(predict)]))\n",
    "                    self.predict_list=tf.concat(0,[self.predict_list,tf.nn.softmax(predict)])\n",
    "                self.predict_list=self.predict_list[1:]\n",
    "                self.cross_entropy_sum=tf.reduce_sum(self.cross_entropy_list)\n",
    "\n",
    "                story_embedding_W_hist=tf.histogram_summary(\"story_embedding_W\",self.story_embedding_W)\n",
    "                #story_embedding_b_hist=tf.histogram_summary(\"story_embedding_b\",self.story_embedding_b)\n",
    "                query_embedding_W_hist=tf.histogram_summary(\"query_embedding_W\",self.query_embedding_W)\n",
    "                #query_embedding_b_hist=tf.histogram_summary(\"query_embedding_b\",self.query_embedding_b)\n",
    "                output_W_hist=tf.histogram_summary(\"output_W\",self.output_W)\n",
    "                #output_b_hist=tf.histogram_summary(\"output_b\",self.output_b)\n",
    "                cross_entropy_sum_scalar = tf.scalar_summary(\"cross_entropy_sum\",self.cross_entropy_sum)\n",
    "                self.merged=tf.merge_all_summaries()\n",
    "                self.writer=tf.train.SummaryWriter(\"/home/asd36952/tfbd/memnet/mymem_korean_hop_batch_search\",self.session.graph)\n",
    "\n",
    "                self.train_step=tf.train.AdamOptimizer(learning_rate=1e-2).minimize(self.cross_entropy_sum)\n",
    "                #self.train_step=tf.contrib.layers.optimize_loss(loss=self.cross_entropy_sum,learning_rate=1e-2,optimizer='Adam',gradient_noise_scale=1e-4,global_step=tf.contrib.framework.get_global_step())\n",
    "                #grads_and_vars = tf.train.AdamOptimizer(learning_rate=1e-2).compute_gradients(self.cross_entropy_sum)\n",
    "                #grads_and_vars = [(tf.clip_by_norm(g, 40), v) for g,v in grads_and_vars]\n",
    "                #self.train_step = tf.train.AdamOptimizer(learning_rate=1e-2).apply_gradients(grads_and_vars)\n",
    "    \n",
    "    def batch_fit(self,train_data,epoch=1000):\n",
    "        train_data,valid_data=self.preprocessing_dict(train_data)\n",
    "        num_train=np.array(train_data).shape[0]\n",
    "        num_valid=np.array(valid_data).shape[0]\n",
    "                                \n",
    "        for ep in range(epoch):\n",
    "            #np.random.shuffle(train_data)\n",
    "            for idx in range(int(num_train/self.batch_size)):\n",
    "                if num_train-idx*self.batch_size<self.batch_size:\n",
    "                    break\n",
    "                batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                #print(np.array(np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist()).shape)\n",
    "                summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                self.writer.add_summary(summary,ep*num_train+idx)\n",
    "            \n",
    "            if (ep+1)%10==0:\n",
    "                total_cross_ent=0\n",
    "                for idx in range(int(num_train/self.batch_size)):\n",
    "                    if num_train-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    \n",
    "                    total_cross_ent+=self.session.run([self.cross_entropy_sum],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})[0]\n",
    "                print(total_cross_ent)\n",
    "                total=0\n",
    "                correct=0\n",
    "                for idx in range(int(num_train/self.batch_size)):\n",
    "                    if num_train-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                    predict_list=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                    #print(predict_list)\n",
    "                    answer_list=np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                    for idx in range(self.batch_size):\n",
    "                        total+=1\n",
    "                        if np.argmax(predict_list[idx])==np.argmax(answer_list[idx]):\n",
    "                            correct+=1\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                print(\"#############################\")\n",
    "                total=0\n",
    "                correct=0\n",
    "                for idx in range(int(num_valid/self.batch_size)):\n",
    "                    if num_valid-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    batch_valid=valid_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                    predict_list=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_valid)[:,0].tolist(),\n",
    "                                              self.query_batch:np.array(np.array(batch_valid)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.answer_batch:np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.memory_init_flag_batch:np.array(batch_valid)[:,3].tolist()})\n",
    "                    #print(predict_list)\n",
    "                    answer_list=np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                    for idx in range(self.batch_size):\n",
    "                        total+=1\n",
    "                        if np.argmax(predict_list[idx])==np.argmax(answer_list[idx]):\n",
    "                            correct+=1\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                print(\"#############################\")\n",
    "                print(\"#############################\")\n",
    "        \n",
    "    def fit(self,train_data,epoch=2000):\n",
    "        train_data,valid_data=self.preprocessing(train_data)\n",
    "        print(\"Learning ...\")\n",
    "        for ep in range(epoch):\n",
    "            for story,query,answer,memory_init_flag in train_data:\n",
    "                #print(story)\n",
    "                summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag})\n",
    "                self.writer.add_summary(summary,ep)\n",
    "            if (ep+1)%10==0:\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in train_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"train\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in valid_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"valid\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        print(\"#############################\")\n",
    "        for story,query,answer,memory_init_flag in valid_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        \n",
    "        \n",
    "        #print(self.session.run([self.embedding_story,self.emdedding_query],feed_dict={self.story:story,self.query:query,self.answer:answer,self.memory_init_flag:memory_init_flag}))\n",
    "    \n",
    "    def preprocessing(self,train_data,validation_cut=0.8):\n",
    "        preprocessed_data=[]\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            story_list=[]\n",
    "            for sentence in story:\n",
    "                sentence_vector=[0]*len(self.vocab)\n",
    "                for word in sentence:\n",
    "                    sentence_vector[self.vocab.index(word)]=1\n",
    "                story_list.append(sentence_vector)\n",
    "            for _ in range(self.story_max-len(story_list)):\n",
    "                story_list.append([0]*len(self.vocab))\n",
    "            query_vector=[0]*len(self.vocab)\n",
    "            for word in query:\n",
    "                query_vector[self.vocab.index(word)]=1\n",
    "            answer_vector=[0]*len(self.vocab)\n",
    "            for word in answer:\n",
    "                answer_vector[self.vocab.index(word)]=1\n",
    "            preprocessed_data.append([story_list,query_vector,answer_vector,memory_init_flag])\n",
    "        #print(preprocessed_data)\n",
    "        return preprocessed_data[:int(len(preprocessed_data)*validation_cut)],preprocessed_data[int(len(preprocessed_data)*validation_cut):]\n",
    "    \n",
    "    def preprocessing_dict(self,train_data,validation_cut=0.8):\n",
    "        preprocessed_data=np.empty([0,4])\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            story_list=np.empty([0,len(self.vocab)])\n",
    "            print(1)\n",
    "            print(len(story))\n",
    "            for sentence in story:\n",
    "                sentence_vector=np.array([0]*len(self.vocab))\n",
    "                for word in sentence:\n",
    "                    sentence_vector[self.vocab_dict[word]]=1\n",
    "                np.append(story_list,sentence_vector.reshape([1,-1]),0)\n",
    "            print(3)\n",
    "            for _ in range(self.story_max-len(story_list)):\n",
    "                np.append(story_list,[0]*len(self.vocab))\n",
    "            query_vector=np.array([0]*len(self.vocab))\n",
    "            print(4)\n",
    "            for word in query:\n",
    "                query_vector[self.vocab_dict[word]]=1\n",
    "            answer_vector=np.array([0]*len(self.vocab))\n",
    "            print(5)\n",
    "            for word in answer:\n",
    "                answer_vector[self.vocab_dict[word]]=1\n",
    "            np.append(preprocessed_data,[[story_list,query_vector,answer_vector,memory_init_flag]],0)\n",
    "        #print(preprocessed_data)\n",
    "        return preprocessed_data[:int(len(preprocessed_data)*validation_cut)],preprocessed_data[int(len(preprocessed_data)*validation_cut):]\n",
    "    \n",
    "    \n",
    "    def init_memory(self):\n",
    "        return tf.constant([[0.0]*self.memory_dim],dtype=tf.float32)\n",
    "        #self.memory=tf.Variable(initial_value=[[0.0]*self.memory_dim]*self.memory_max,trainable=False)\n",
    "        #self.memory=[[0.0]*self.memory_dim]*self.memory_max\n",
    "        #self.memory_pointer=tf.constant(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.array([[1,2,3],[4,5,6,],[7,8,9]])\n",
    "b=np.array([1,2,3])\n",
    "c=np.array([4,5,6])\n",
    "print(np.sum(a,1))\n",
    "print(np.sum(b*c[:,np.newaxis],1))\n",
    "print((np.identity(3)-c))\n",
    "print((np.identity(3)-c)*b[:,np.newaxis])\n",
    "print(np.sum((np.identity(3)-c)*b[:,np.newaxis],1))\n",
    "print(b*c)\n",
    "print(b*c[:,np.newaxis])\n",
    "print(a.T)\n",
    "print(1-b)\n",
    "#a.put(np.array([4,4,4]))\n",
    "#print(a)\n",
    "print(3.6460299491882324*432/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_utils import load_task\n",
    "from itertools import chain\n",
    "ids = range(1, 21)\n",
    "train, test = [], []\n",
    "for i in ids:\n",
    "    train+= load_task(\"data/tasks_1-20_v1-2/en/\", i)[0]\n",
    "    test+=load_task(\"data/tasks_1-20_v1-2/en/\", i)[1]\n",
    "    #train.append(tr)\n",
    "    #test.append(te)\n",
    "#data = list(chain.from_iterable(train + test))\n",
    "#print(train[0])\n",
    "#train, test = load_task(\"data/tasks_1-20_v1-2/en/\", 1)\n",
    "#print(train)\n",
    "vocab=[]\n",
    "story_max=0\n",
    "train_new=[]\n",
    "for story,query,answer in train:\n",
    "    story_max=max(story_max,len(story))\n",
    "    for sent in story:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    for word in query:\n",
    "        vocab.append(word)\n",
    "    for word in answer:\n",
    "        vocab.append(word)\n",
    "    train_new.append([story,query,answer,1])\n",
    "vocab=list(set(vocab))\n",
    "print(train_new[0])\n",
    "print(len(train_new))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a=MN(vocab,20,300,story_max,3,100,sess)\n",
    "    a.batch_fit(train_new,epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "class MN():\n",
    "    def __init__(self,vocab,memory_dim,memory_max,story_max,hop,batch_size,session):\n",
    "        self.vocab=vocab\n",
    "        self.vocab_dict=dict()\n",
    "        for idx in range(len(vocab)):\n",
    "            self.vocab_dict[vocab[idx]]=idx+1\n",
    "        \n",
    "        self.story_max=story_max\n",
    "        \n",
    "        self.hop=hop\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.memory_max=memory_max\n",
    "        self.memory_dim=memory_dim\n",
    "        #self.init_memory()\n",
    "        self.memory=self.init_memory()\n",
    "        \n",
    "        #self.initializer=tf.random_normal_initializer(stddev=0.1)\n",
    "        self.session=session\n",
    "        self.init_tf_variable()\n",
    "        \n",
    "        init_tf=tf.initialize_all_variables()\n",
    "        \n",
    "        self.session.run(init_tf)\n",
    "        \n",
    "    def return_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def init_tf_variable(self):\n",
    "        #self.story_embedding_W=tf.Variable(self.initializer)\n",
    "        self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"story_embedding_W\")\n",
    "        #self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"story_embedding_W\")\n",
    "        #self.story_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"story_embedding_b\")\n",
    "        \n",
    "        self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"query_embedding_W\")\n",
    "        #self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"query_embedding_W\")\n",
    "        #self.query_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"query_embedding_b\")\n",
    "        \n",
    "        self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=1.0/math.sqrt(float(self.memory_dim))),name=\"output_W\")\n",
    "        #self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=0.1),name=\"output_W\")\n",
    "        #self.output_b=tf.Variable(tf.zeros([len(self.vocab)]),name=\"output_b\")\n",
    "        \n",
    "        self.TA = tf.Variable(tf.truncated_normal([self.story_max,self.memory_dim],stddev=0.1))\n",
    "        \n",
    "        self.story_batch=tf.placeholder(tf.float32,[self.batch_size,self.story_max,len(self.vocab)])\n",
    "        self.query_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "        self.answer_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "        self.memory_init_flag_batch=tf.placeholder(tf.int32,[self.batch_size])\n",
    "        \n",
    "        \n",
    "        self.cross_entropy_sum=tf.constant(0.0)\n",
    "        self.predict_list=tf.constant([[0.0]*len(self.vocab)])\n",
    "        self.cross_entropy_list=tf.constant([])\n",
    "        for idx in range(self.batch_size):\n",
    "            #print(tf.constant(1))\n",
    "            #print()\n",
    "            \n",
    "            #self.memory=tf.mul(self.memory,tf.sub(tf.constant(1.0),tf.cast(self.memory_init_flag_batch[idx],dtype=tf.float32)))\n",
    "            #self.memory_pointer=tf.mul(self.memory_pointer,tf.sub(tf.constant(1),self.memory_init_flag_batch[idx]))\n",
    "            #self.embedding_story=tf.matmul(self.story_batch[idx],self.story_embedding_W)+self.story_embedding_b\n",
    "            #embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)+self.query_embedding_b\n",
    "            #self.memory=tf.cond(tf.equal(self.memory_init_flag_batch[idx],tf.constant(1)),self.init_memory,self.return_memory)\n",
    "            self.embedding_story=tf.add(tf.matmul(self.story_batch[idx],self.story_embedding_W),self.TA) \n",
    "            embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)\n",
    "            #tf.scatter_update(self.memory,self.memory_pointer,self.embedding_story)\n",
    "            \n",
    "            #self.memory=tf.concat(0,[self.memory,self.embedding_story])\n",
    "            \"\"\"\n",
    "            for elem in tf.split(0,self.embedding_story.get_shape()[0],self.embedding_story):\n",
    "                #print(self.memory)\n",
    "                self.memory=tf.concat(0,[self.memory,elem])\n",
    "                #self.memorytf.reshape(elem,[-1])\n",
    "                #self.memory_pointer=tf.add(self.memory_pointer,1)\n",
    "            \"\"\"\n",
    "            #self.memory=tf.concat(self.memory,self.embedding_story)\n",
    "            self.u=embedding_query\n",
    "            for i in range(self.hop):\n",
    "                \n",
    "\n",
    "                \n",
    "                self.attention=tf.nn.softmax(tf.reshape(tf.reduce_sum(tf.mul(self.embedding_story,self.u),1),[1,-1]))\n",
    "                #self.attention=tf.nn.softmax(tf.reshape(tf.reduce_sum(tf.mul(self.memory[1:],self.u),1),[1,-1]))\n",
    "                self.attended_embedding=tf.matmul(self.attention,self.embedding_story)\n",
    "                #self.attended_embedding=tf.matmul(self.attention,self.memory[1:])\n",
    "                self.u=tf.add(self.u,self.attended_embedding)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #self.predict=self.attended_embedding+self.embedding_query\n",
    "                #self.predict=tf.matmul(self.attended_embedding+self.embedding_query,self.output_W)+self.output_b\n",
    "            #predict=tf.matmul(self.u,self.output_W)+self.output_b\n",
    "            predict=tf.matmul(self.u,self.output_W)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(predict, tf.cast(self.answer_batch[idx], tf.float32), name=\"cross_entropy\")\n",
    "            self.cross_entropy_list=tf.concat(0,[self.cross_entropy_list,cross_entropy])\n",
    "            #self.cross_entropy_sum=tf.add(self.cross_entropy_sum,tf.reduce_sum(cross_entropy))\n",
    "          \n",
    "            #self.predict_list=tf.cond(tf.equal(self.predict_list,tf.constant([[]])),\n",
    "            #self.predict_list=tf.cond(tf.constant(self.predict_list==tf.constant([[]])),\n",
    "            #                          lambda:tf.concat(1,[self.predict_list,tf.nn.softmax(predict)]),lambda:tf.concat(0,[self.predict_list,tf.nn.softmax(predict)]))\n",
    "            self.predict_list=tf.concat(0,[self.predict_list,tf.nn.softmax(predict)])\n",
    "        self.predict_list=self.predict_list[1:]\n",
    "        self.cross_entropy_sum=tf.reduce_sum(self.cross_entropy_list)\n",
    "        \n",
    "        story_embedding_W_hist=tf.histogram_summary(\"story_embedding_W\",self.story_embedding_W)\n",
    "        #story_embedding_b_hist=tf.histogram_summary(\"story_embedding_b\",self.story_embedding_b)\n",
    "        query_embedding_W_hist=tf.histogram_summary(\"query_embedding_W\",self.query_embedding_W)\n",
    "        #query_embedding_b_hist=tf.histogram_summary(\"query_embedding_b\",self.query_embedding_b)\n",
    "        output_W_hist=tf.histogram_summary(\"output_W\",self.output_W)\n",
    "        #output_b_hist=tf.histogram_summary(\"output_b\",self.output_b)\n",
    "        cross_entropy_sum_scalar = tf.scalar_summary(\"cross_entropy_sum\",self.cross_entropy_sum)\n",
    "        self.merged=tf.merge_all_summaries()\n",
    "        self.writer=tf.train.SummaryWriter(\"/home/asd36952/tfbd/memnet/mymem_babi_hop_batch\",self.session.graph)\n",
    "        \n",
    "        self.train_step=tf.train.AdamOptimizer(learning_rate=1e-2).minimize(self.cross_entropy_sum)\n",
    "        #self.train_step=tf.contrib.layers.optimize_loss(loss=self.cross_entropy_sum,learning_rate=1e-2,optimizer='Adam',gradient_noise_scale=1e-4,global_step=tf.contrib.framework.get_global_step())\n",
    "        #grads_and_vars = tf.train.AdamOptimizer(learning_rate=1e-2).compute_gradients(self.cross_entropy_sum)\n",
    "        #grads_and_vars = [(tf.clip_by_norm(g, 40), v) for g,v in grads_and_vars]\n",
    "        #self.train_step = tf.train.AdamOptimizer(learning_rate=1e-2).apply_gradients(grads_and_vars)\n",
    "    \n",
    "    def batch_pred(self,data):\n",
    "        data, _ = self.preprocessing(data,1)\n",
    "        num_data=np.array(data).shape[0]\n",
    "        predict_list=[]\n",
    "        for idx in range(int(num_data/self.batch_size)):\n",
    "            batch_data=data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "            predict_list+=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "        return predict_list\n",
    "    \n",
    "    def batch_fit(self,train_data,epoch=1000):\n",
    "        train_data,valid_data=self.preprocessing(train_data)\n",
    "        num_train=np.array(train_data).shape[0]\n",
    "        num_valid=np.array(valid_data).shape[0]\n",
    "                                \n",
    "        for ep in range(epoch):\n",
    "            #np.random.shuffle(train_data)\n",
    "            for idx in range(int(num_train/self.batch_size)):\n",
    "                if num_train-idx*self.batch_size<self.batch_size:\n",
    "                    break\n",
    "                batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                #print(np.array(np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist()).shape)\n",
    "                summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                self.writer.add_summary(summary,ep*num_train+idx)\n",
    "            \n",
    "\n",
    "            total_cross_ent=0\n",
    "            for idx in range(int(num_train/self.batch_size)):\n",
    "                if num_train-idx*self.batch_size<self.batch_size:\n",
    "                    break\n",
    "\n",
    "                total_cross_ent+=self.session.run([self.cross_entropy_sum],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                          self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                          self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                          self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})[0]\n",
    "            print(total_cross_ent)\n",
    "            total=0\n",
    "            correct=0\n",
    "            for idx in range(int(num_train/self.batch_size)):\n",
    "                if num_train-idx*self.batch_size<self.batch_size:\n",
    "                    break\n",
    "                batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                predict_list=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                          self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                          self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                          self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                #print(predict_list)\n",
    "                answer_list=np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                for idx in range(self.batch_size):\n",
    "                    total+=1\n",
    "                    if np.argmax(predict_list[idx])==np.argmax(answer_list[idx]):\n",
    "                        correct+=1\n",
    "            print(total)\n",
    "            print(correct)\n",
    "            print(correct/total)\n",
    "            print(\"#############################\")\n",
    "            total=0\n",
    "            correct=0\n",
    "            for idx in range(int(num_valid/self.batch_size)):\n",
    "                if num_valid-idx*self.batch_size<self.batch_size:\n",
    "                    break\n",
    "                batch_valid=valid_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                predict_list=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_valid)[:,0].tolist(),\n",
    "                                          self.query_batch:np.array(np.array(batch_valid)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                          self.answer_batch:np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                          self.memory_init_flag_batch:np.array(batch_valid)[:,3].tolist()})\n",
    "                #print(predict_list)\n",
    "                answer_list=np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                for idx in range(self.batch_size):\n",
    "                    total+=1\n",
    "                    if np.argmax(predict_list[idx])==np.argmax(answer_list[idx]):\n",
    "                        correct+=1\n",
    "            print(total)\n",
    "            print(correct)\n",
    "            print(correct/total)\n",
    "            print(\"#############################\")\n",
    "            print(\"#############################\")\n",
    "        \n",
    "    def fit(self,train_data,epoch=2000):\n",
    "        train_data,valid_data=self.preprocessing(train_data)\n",
    "        print(\"Learning ...\")\n",
    "        for ep in range(epoch):\n",
    "            for story,query,answer,memory_init_flag in train_data:\n",
    "                #print(story)\n",
    "                summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag})\n",
    "                self.writer.add_summary(summary,ep)\n",
    "            if (ep+1)%10==0:\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in train_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"train\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in valid_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"valid\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        print(\"#############################\")\n",
    "        for story,query,answer,memory_init_flag in valid_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        \n",
    "        \n",
    "        #print(self.session.run([self.embedding_story,self.emdedding_query],feed_dict={self.story:story,self.query:query,self.answer:answer,self.memory_init_flag:memory_init_flag}))\n",
    "    \n",
    "    def preprocessing(self,train_data,validation_cut=0.8):\n",
    "        preprocessed_data=[]\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            story_list=[]\n",
    "            for sentence in story:\n",
    "                sentence_vector=[0]*len(self.vocab)\n",
    "                for word in sentence:\n",
    "                    sentence_vector[self.vocab.index(word)]=1\n",
    "                story_list.append(sentence_vector)\n",
    "            for _ in range(self.story_max-len(story_list)):\n",
    "                story_list.append([0]*len(self.vocab))\n",
    "            query_vector=[0]*len(self.vocab)\n",
    "            for word in query:\n",
    "                query_vector[self.vocab.index(word)]=1\n",
    "            answer_vector=[0]*len(self.vocab)\n",
    "            for word in answer:\n",
    "                answer_vector[self.vocab.index(word)]=1\n",
    "            preprocessed_data.append([story_list,query_vector,answer_vector,memory_init_flag])\n",
    "        #print(preprocessed_data)\n",
    "        return preprocessed_data[:int(len(preprocessed_data)*validation_cut)],preprocessed_data[int(len(preprocessed_data)*validation_cut):]\n",
    "            \n",
    "    \n",
    "    def init_memory(self):\n",
    "        return tf.constant([[0.0]*self.memory_dim],dtype=tf.float32)\n",
    "        #self.memory=tf.Variable(initial_value=[[0.0]*self.memory_dim]*self.memory_max,trainable=False)\n",
    "        #self.memory=[[0.0]*self.memory_dim]*self.memory_max\n",
    "        #self.memory_pointer=tf.constant(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
