{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import util\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "\n",
    "\n",
    "queries=util.load_our_queries_morp()\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=[]\n",
    "for idx in range(len(queries)):\n",
    "    train.append([util.load_stories_with_our_query(queries[idx]),queries[idx][0],queries[idx][1],1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6677\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "vocab=[]\n",
    "story_max=0\n",
    "\n",
    "for stories,query,answer,flag in train:\n",
    "    story_max=max(story_max,len(stories))\n",
    "    for sent in stories:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    for word in query:\n",
    "        vocab.append(word)\n",
    "    for word in answer:\n",
    "        vocab.append(word)\n",
    "vocab=list(set(vocab))\n",
    "\n",
    "print(len(vocab))\n",
    "print(story_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "preprocessing start!\n",
      "preprocessing finish!: 8.971694469451904\n",
      "27.6780459881\n",
      "112\n",
      "99\n",
      "0.8839285714285714\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "483.617103577\n",
      "112\n",
      "100\n",
      "0.8928571428571429\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "145.083654404\n",
      "112\n",
      "107\n",
      "0.9553571428571429\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "0.000648125966109\n",
      "112\n",
      "112\n",
      "1.0\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "0.0131081994623\n",
      "112\n",
      "112\n",
      "1.0\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "0.0\n",
      "112\n",
      "112\n",
      "1.0\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "0.00644087622641\n",
      "112\n",
      "112\n",
      "1.0\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "0.00532969809137\n",
      "112\n",
      "112\n",
      "1.0\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "0.0\n",
      "112\n",
      "112\n",
      "1.0\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "0.0033843377314\n",
      "112\n",
      "112\n",
      "1.0\n",
      "#############################\n",
      "24\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "with tf.Graph().as_default(),tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options,allow_soft_placement = True)) as sess:\n",
    "    print(1)\n",
    "    a=MN(vocab,512,300,story_max,3,batch_size,sess)\n",
    "    print(2)\n",
    "    a.batch_fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import util\n",
    "import pickle\n",
    "\n",
    "\n",
    "class MN():\n",
    "    def __init__(self,vocab,memory_dim,memory_max,story_max,hop,batch_size,session):\n",
    "        self.vocab=vocab\n",
    "        \n",
    "        self.story_max=story_max\n",
    "        \n",
    "        self.hop=hop\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.memory_max=memory_max\n",
    "        self.memory_dim=memory_dim\n",
    "        #self.init_memory()\n",
    "        self.memory=self.init_memory()\n",
    "        \n",
    "        #self.initializer=tf.random_normal_initializer(stddev=0.1)\n",
    "        self.session=session\n",
    "        self.init_tf_variable()\n",
    "        \n",
    "        init_tf=tf.initialize_all_variables()\n",
    "        \n",
    "        self.session.run(init_tf)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def return_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def init_tf_variable(self):\n",
    "        #self.story_embedding_W=tf.Variable(self.initializer)\n",
    "        self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"story_embedding_W\")\n",
    "        #self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"story_embedding_W\")\n",
    "        #self.story_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"story_embedding_b\")\n",
    "        \n",
    "        self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"query_embedding_W\")\n",
    "        #self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"query_embedding_W\")\n",
    "        #self.query_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"query_embedding_b\")\n",
    "        \n",
    "        self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=1.0/math.sqrt(float(self.memory_dim))),name=\"output_W\")\n",
    "        #self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=0.1),name=\"output_W\")\n",
    "        #self.output_b=tf.Variable(tf.zeros([len(self.vocab)]),name=\"output_b\")\n",
    "        \n",
    "        self.TA = tf.Variable(tf.truncated_normal([self.story_max,self.memory_dim],stddev=0.1))\n",
    "        \n",
    "        self.story_batch=tf.placeholder(tf.float32,[self.batch_size,self.story_max,len(self.vocab)])\n",
    "        self.query_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "        self.answer_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "        self.memory_init_flag_batch=tf.placeholder(tf.int32,[self.batch_size])\n",
    "        \n",
    "        \n",
    "        self.cross_entropy_sum=tf.constant(0.0)\n",
    "        self.predict_list=tf.constant([[0.0]*len(self.vocab)])\n",
    "        self.cross_entropy_list=tf.constant([])\n",
    "        for idx in range(self.batch_size):\n",
    "            #print(tf.constant(1))\n",
    "            #print()\n",
    "            \n",
    "            #self.memory=tf.mul(self.memory,tf.sub(tf.constant(1.0),tf.cast(self.memory_init_flag_batch[idx],dtype=tf.float32)))\n",
    "            #self.memory_pointer=tf.mul(self.memory_pointer,tf.sub(tf.constant(1),self.memory_init_flag_batch[idx]))\n",
    "            #self.embedding_story=tf.matmul(self.story_batch[idx],self.story_embedding_W)+self.story_embedding_b\n",
    "            #embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)+self.query_embedding_b\n",
    "            #self.memory=tf.cond(tf.equal(self.memory_init_flag_batch[idx],tf.constant(1)),self.init_memory,self.return_memory)\n",
    "            self.embedding_story=tf.add(tf.matmul(self.story_batch[idx],self.story_embedding_W),self.TA) \n",
    "            embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)\n",
    "            #tf.scatter_update(self.memory,self.memory_pointer,self.embedding_story)\n",
    "            \n",
    "            #self.memory=tf.concat(0,[self.memory,self.embedding_story])\n",
    "            \"\"\"\n",
    "            for elem in tf.split(0,self.embedding_story.get_shape()[0],self.embedding_story):\n",
    "                #print(self.memory)\n",
    "                self.memory=tf.concat(0,[self.memory,elem])\n",
    "                #self.memorytf.reshape(elem,[-1])\n",
    "                #self.memory_pointer=tf.add(self.memory_pointer,1)\n",
    "            \"\"\"\n",
    "            #self.memory=tf.concat(self.memory,self.embedding_story)\n",
    "            self.u=embedding_query\n",
    "            for i in range(self.hop):\n",
    "                \n",
    "\n",
    "                \n",
    "                self.attention=tf.nn.softmax(tf.reshape(tf.reduce_sum(tf.mul(self.embedding_story,self.u),1),[1,-1]))\n",
    "                #self.attention=tf.nn.softmax(tf.reshape(tf.reduce_sum(tf.mul(self.memory[1:],self.u),1),[1,-1]))\n",
    "                self.attended_embedding=tf.matmul(self.attention,self.embedding_story)\n",
    "                #self.attended_embedding=tf.matmul(self.attention,self.memory[1:])\n",
    "                self.u=tf.add(self.u,self.attended_embedding)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #self.predict=self.attended_embedding+self.embedding_query\n",
    "                #self.predict=tf.matmul(self.attended_embedding+self.embedding_query,self.output_W)+self.output_b\n",
    "            #predict=tf.matmul(self.u,self.output_W)+self.output_b\n",
    "            predict=tf.matmul(self.u,self.output_W)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(predict, tf.cast(self.answer_batch[idx], tf.float32), name=\"cross_entropy\")\n",
    "            self.cross_entropy_list=tf.concat(0,[self.cross_entropy_list,cross_entropy])\n",
    "            #self.cross_entropy_sum=tf.add(self.cross_entropy_sum,tf.reduce_sum(cross_entropy))\n",
    "          \n",
    "            #self.predict_list=tf.cond(tf.equal(self.predict_list,tf.constant([[]])),\n",
    "            #self.predict_list=tf.cond(tf.constant(self.predict_list==tf.constant([[]])),\n",
    "            #                          lambda:tf.concat(1,[self.predict_list,tf.nn.softmax(predict)]),lambda:tf.concat(0,[self.predict_list,tf.nn.softmax(predict)]))\n",
    "            self.predict_list=tf.concat(0,[self.predict_list,tf.nn.softmax(predict)])\n",
    "        self.predict_list=self.predict_list[1:]\n",
    "        self.cross_entropy_sum=tf.reduce_sum(self.cross_entropy_list)\n",
    "        \n",
    "        story_embedding_W_hist=tf.histogram_summary(\"story_embedding_W\",self.story_embedding_W)\n",
    "        #story_embedding_b_hist=tf.histogram_summary(\"story_embedding_b\",self.story_embedding_b)\n",
    "        query_embedding_W_hist=tf.histogram_summary(\"query_embedding_W\",self.query_embedding_W)\n",
    "        #query_embedding_b_hist=tf.histogram_summary(\"query_embedding_b\",self.query_embedding_b)\n",
    "        output_W_hist=tf.histogram_summary(\"output_W\",self.output_W)\n",
    "        #output_b_hist=tf.histogram_summary(\"output_b\",self.output_b)\n",
    "        cross_entropy_sum_scalar = tf.scalar_summary(\"cross_entropy_sum\",self.cross_entropy_sum)\n",
    "        self.merged=tf.merge_all_summaries()\n",
    "        self.writer=tf.train.SummaryWriter(\"/home/asd36952/tfbd/memnet/mymem_our_data_hop_batch\",self.session.graph)\n",
    "        \n",
    "        self.train_step=tf.train.AdamOptimizer(learning_rate=1e-2).minimize(self.cross_entropy_sum)\n",
    "                #self.train_step=tf.contrib.layers.optimize_loss(loss=self.cross_entropy_sum,learning_rate=1e-2,optimizer='Adam',gradient_noise_scale=1e-4,global_step=tf.contrib.framework.get_global_step())\n",
    "                #grads_and_vars = tf.train.AdamOptimizer(learning_rate=1e-2).compute_gradients(self.cross_entropy_sum)\n",
    "                #grads_and_vars = [(tf.clip_by_norm(g, 40), v) for g,v in grads_and_vars]\n",
    "                #self.train_step = tf.train.AdamOptimizer(learning_rate=1e-2).apply_gradients(grads_and_vars)\n",
    "    \n",
    "    def batch_fit(self,train_data,epoch=100):\n",
    "        tick=time.time()\n",
    "        print(\"preprocessing start!\")\n",
    "        train_data,valid_data=self.preprocessing(train_data)\n",
    "        print(\"preprocessing finish!:\",time.time()-tick)\n",
    "        num_train=np.array(train_data).shape[0]\n",
    "        num_valid=np.array(valid_data).shape[0]\n",
    "                                \n",
    "        for ep in range(epoch):\n",
    "            np.random.shuffle(train_data)\n",
    "            for idx in range(int(num_train/self.batch_size)):\n",
    "                current_time=time.time()\n",
    "                if num_train-idx*self.batch_size<self.batch_size:\n",
    "                    break\n",
    "                batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                #print(np.array(batch_train)[:,0].tolist())\n",
    "                #print(np.array(np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist()).shape)\n",
    "                summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                self.writer.add_summary(summary,ep*num_train+idx)\n",
    "            if (ep+1)%10==0:\n",
    "                total_cross_ent=0\n",
    "                for idx in range(int(num_train/self.batch_size)):\n",
    "                    if num_train-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    \n",
    "                    total_cross_ent+=self.session.run([self.cross_entropy_sum],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})[0]\n",
    "                print(total_cross_ent)\n",
    "                total=0\n",
    "                correct=0\n",
    "                for idx in range(int(num_train/self.batch_size)):\n",
    "                    if num_train-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                    predict_list=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                    #print(predict_list)\n",
    "                    answer_list=np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                    for idx in range(self.batch_size):\n",
    "                        total+=1\n",
    "                        if np.argmax(predict_list[idx])==np.argmax(answer_list[idx]):\n",
    "                            correct+=1\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                print(\"#############################\")\n",
    "                total=0\n",
    "                correct=0\n",
    "                for idx in range(int(num_valid/self.batch_size)):\n",
    "                    if num_valid-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    batch_valid=valid_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                    predict_list=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_valid)[:,0].tolist(),\n",
    "                                              self.query_batch:np.array(np.array(batch_valid)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.answer_batch:np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.memory_init_flag_batch:np.array(batch_valid)[:,3].tolist()})\n",
    "                    #print(predict_list)\n",
    "                    answer_list=np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                    for idx in range(self.batch_size):\n",
    "                        total+=1\n",
    "                        if np.argmax(predict_list[idx])==np.argmax(answer_list[idx]):\n",
    "                            correct+=1\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                print(\"#############################\")\n",
    "                print(\"#############################\")\n",
    "        \n",
    "    def fit(self,train_data,epoch=2000):\n",
    "        train_data,valid_data=self.preprocessing(train_data)\n",
    "        print(\"Learning ...\")\n",
    "        for ep in range(epoch):\n",
    "            for story,query,answer,memory_init_flag in train_data:\n",
    "                #print(story)\n",
    "                summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag})\n",
    "                self.writer.add_summary(summary,ep)\n",
    "            if (ep+1)%10==0:\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in train_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"train\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in valid_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"valid\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        print(\"#############################\")\n",
    "        for story,query,answer,memory_init_flag in valid_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        \n",
    "        \n",
    "        #print(self.session.run([self.embedding_story,self.emdedding_query],feed_dict={self.story:story,self.query:query,self.answer:answer,self.memory_init_flag:memory_init_flag}))\n",
    "    \n",
    "    def preprocessing(self,train_data,validation_cut=0.8):\n",
    "        preprocessed_data=[]\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            story_list=[]\n",
    "            for sentence in story:\n",
    "                sentence_vector=[0]*len(self.vocab)\n",
    "                for word in sentence:\n",
    "                    sentence_vector[self.vocab.index(word)]=1\n",
    "                story_list.append(sentence_vector)\n",
    "            for _ in range(self.story_max-len(story_list)):\n",
    "                story_list.append([0]*len(self.vocab))\n",
    "            query_vector=[0]*len(self.vocab)\n",
    "            for word in query:\n",
    "                query_vector[self.vocab.index(word)]=1\n",
    "            answer_vector=[0]*len(self.vocab)\n",
    "            for word in answer:\n",
    "                answer_vector[self.vocab.index(word)]=1\n",
    "            preprocessed_data.append([story_list,query_vector,answer_vector,memory_init_flag])\n",
    "        #print(preprocessed_data)\n",
    "        return preprocessed_data[:int(len(preprocessed_data)*validation_cut)],preprocessed_data[int(len(preprocessed_data)*validation_cut):]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_memory(self):\n",
    "        return tf.constant([[0.0]*self.memory_dim],dtype=tf.float32)\n",
    "        #self.memory=tf.Variable(initial_value=[[0.0]*self.memory_dim]*self.memory_max,trainable=False)\n",
    "        #self.memory=[[0.0]*self.memory_dim]*self.memory_max\n",
    "        #self.memory_pointer=tf.constant(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.empty([0,3])\n",
    "b=np.array([1,2,3])\n",
    "c=np.array([1,2,3,4])\n",
    "d=np.array([1,2,3,4,5])\n",
    "print(np.append(a,np.array([[b,c,d]]),0))\n",
    "#print(np.append(np.append(a,b,0),b,0))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babi_example=[[[\"Mary moved to the bathroom.\",\"John went to the hallway.\"],\"Where is Mary?\", \"bathroom\",0],\n",
    "              [[\"Daniel went back to the hallway.\",\"Sandra moved to the garden.\"],\"Where is Daniel?\",\"hallway\",1],\n",
    "              [[\"John moved to the office\",\"Sandra journeyed to the bathroom.\"],\"Where is Daniel?\",\"hallway\",1],\n",
    "              [[\"Mary moved to the hallway.\",\"Daniel travelled to the office.\"],\"Where is Daniel?\",\"office\",1],\n",
    "              [[\"John went back to the garden.\",\"John moved to the bedroom.\"],\"Where is Sandra?\",\"bathroom\",1],\n",
    "              [[\"Sandra travelled to the office.\",\"Sandra went to the bathroom.\"],\"Where is Sandra?\", \"bathroom\",0],\n",
    "              [[\"Mary went to the bedroom.\",\"Daniel moved to the hallway.\"],\"Where is Sandra?\", \"bathroom\",1],\n",
    "              [[\"John went to the garden.\",\"John travelled to the office.\"],\"Where is Sandra?\",  \"bathroom\",1],    \n",
    "              [[\"Daniel journeyed to the bedroom.\",\"Daniel travelled to the hallway.\"], \"Where is John?\", \"office\",1],\n",
    "              [[\"John went to the bedroom.\",\"John travelled to the office.\"],\"Where is Daniel?\", \"hallway\",1]]\n",
    "preprocessed=[]\n",
    "vocab=[]\n",
    "for story,query,answer,memory_init_flag in babi_example:\n",
    "    sentence_list=[]\n",
    "    for sentence in story:\n",
    "        sentence_list.append(nltk.word_tokenize(sentence))\n",
    "    for sentence in sentence_list:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    query_list=nltk.word_tokenize(query)\n",
    "    for word in query_list:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "    answer_list=nltk.word_tokenize(answer)\n",
    "    for word in answer_list:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "    preprocessed.append([sentence_list,query_list,answer_list,memory_init_flag])\n",
    "print(preprocessed[0])\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    a=MN(vocab,10,30,2,sess)\n",
    "    a.fit(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.41786873+0.61164886\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([[[[0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 0], [[[0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "d=[[[tf.constant(0),tf.constant(0)],[tf.constant(0),tf.constant(0)]]]*2\n",
    "with tf.device('/gpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.mul(a, b)\n",
    "  d[0]=c\n",
    "  e = tf.mul(d,tf.constant(3.0))\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "d=[[tf.constant(0.0)]*3]*5\n",
    "with tf.device('/gpu:0'):\n",
    "  a = tf.constant([[[1.0, 2.0, 3.0,4.0],[5.0,6.0,7.0,8.0],[8.0,9.0,10.0,11.0]]], name='a')\n",
    "  b = tf.constant([[3.0, 4.0, 5.0]], name='b')\n",
    "  #d[0]=tf.cast(a,tf.float32)\n",
    "  f=tf.matmul(b,a)\n",
    "  #e = a*[tf.constant(3.0),tf.constant(4.0),tf.constant(5.0)]\n",
    "  e = tf.matmul(([[tf.constant(3.0),tf.constant(4.0),tf.constant(5.0)]]),a)\n",
    "  #g=tf.reduce_sum(a*b,1)\n",
    "  h=b+b\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(e))\n",
    "print(sess.run(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant(5.0, shape=[5, 6])\n",
    "w = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "xw = tf.mul(x, w)\n",
    "max_in_rows = tf.reduce_max(xw, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(xw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
