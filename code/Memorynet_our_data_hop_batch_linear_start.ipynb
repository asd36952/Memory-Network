{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import util\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "\n",
    "\n",
    "queries=util.load_our_queries_morp()\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['천왕/NNG', '성/NNG', '의/JKG', '마지막/NNG', '주요/NNG', '위성/NNG', '미란다/NNP', '를/JKO', '발견/NNG', '하/XSV', 'ㄴ/ETM', '연도/NNG', '는/JX', '?/SF'], ['1948/SN'], 5]\n"
     ]
    }
   ],
   "source": [
    "print(queries[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=[]\n",
    "for idx in range(len(queries)):\n",
    "    train.append([util.load_stories_with_our_query(queries[idx]),queries[idx][0],queries[idx][1],1,queries[idx][2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['팬젠더/NNP', '.'], ['팬젠더/NNP', '(/SS', 'pangender/SL', ')/SS', '는/JX', '자신/NNG', '이/JKS', '모든/MM', '젠더/NNG', '에/JKB', '속/NNG', '하/XSV', 'ㄴ다고/EC', '자각/NNG', '하/XSV', '는/ETM', '정체/NNG', '성/XSN', '이/VCP', '다/EF', './SF', '.'], ['자신/NNG', '이/JKS', '여러/MM', '가지/NNB', '젠더/NNG', '를/JKO', '오가/VV', 'ㄴ다고/EC', '자각/NNG', '하/XSV', '는/ETM', '젠더플루이드/NNP', '와/JKB', '는/JX', '완전히/MAG', '일치/NNG', '하/XSV', '지/EC', '않/VX', '으며/EC', '어느/MM', '정도/NNG', '구분/NNG', '되/XSV', 'ㄴ다/EF', './SF', '.'], ['</SS', 'br/SL', '//SP', '>/SS', '분류/NNG', ':/SP', '사회/NNG', '적/XSN', '성/NNG', ',/SP', '젠더퀴/NNP', '어/EC', '.']], ['자신/NNG', '이/JKS', '모든/MM', '젠더/NNG', '에/JKB', '속/NNG', '하/XSV', 'ㄴ다고/EC', '자각/NNG', '하/XSV', '는/ETM', '정체/NNG', '성/XSN', '은/JX', '무엇/NP', '이/VCP', 'ㄴ가/EF', '?/SF'], ['팬젠더/NNP'], 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14071\n",
      "169\n"
     ]
    }
   ],
   "source": [
    "vocab=[]\n",
    "story_max=0\n",
    "\n",
    "for stories,query,answer,flag,idx in train:\n",
    "    story_max=max(story_max,len(stories))\n",
    "    for sent in stories:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    for word in query:\n",
    "        vocab.append(word)\n",
    "    for word in answer:\n",
    "        vocab.append(word)\n",
    "vocab=list(set(vocab))\n",
    "\n",
    "print(len(vocab))\n",
    "print(story_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model ...\n",
      "Start training ...\n",
      "epoch: 10\n",
      "total loss: 1483.50985527\n",
      "232\n",
      "87\n",
      "0.375\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 20\n",
      "total loss: 938.493318558\n",
      "232\n",
      "117\n",
      "0.5043103448275862\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 30\n",
      "total loss: 1436.18235064\n",
      "232\n",
      "116\n",
      "0.5\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 40\n",
      "total loss: 48.9592175999\n",
      "232\n",
      "230\n",
      "0.9913793103448276\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 50\n",
      "total loss: 0.220176851493\n",
      "232\n",
      "232\n",
      "1.0\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 60\n",
      "total loss: 0.106906928762\n",
      "232\n",
      "232\n",
      "1.0\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 70\n",
      "total loss: 1531.77592107\n",
      "232\n",
      "200\n",
      "0.8620689655172413\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 80\n",
      "total loss: 31.9800050367\n",
      "232\n",
      "229\n",
      "0.9870689655172413\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 90\n",
      "total loss: 0.0698034593592\n",
      "232\n",
      "232\n",
      "1.0\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n",
      "epoch: 100\n",
      "total loss: 0.0466299473487\n",
      "232\n",
      "232\n",
      "1.0\n",
      "#############################\n",
      "56\n",
      "0\n",
      "0.0\n",
      "#############################\n",
      "#############################\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "with tf.Graph().as_default(),tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options,allow_soft_placement = True)) as sess:\n",
    "    print(\"Build model ...\")\n",
    "    a=MN(vocab,64,300,story_max,3,batch_size,sess)\n",
    "    print(\"Start training ...\")\n",
    "    a.batch_fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import util\n",
    "import pickle\n",
    "\n",
    "\n",
    "class MN():\n",
    "    def __init__(self,vocab,memory_dim,memory_max,story_max,hop,batch_size,session):\n",
    "        self.vocab=vocab\n",
    "        \n",
    "        self.story_max=story_max\n",
    "        \n",
    "        self.hop=hop\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.memory_max=memory_max\n",
    "        self.memory_dim=memory_dim\n",
    "        #self.init_memory()\n",
    "        self.memory=self.init_memory()\n",
    "        \n",
    "        #self.initializer=tf.random_normal_initializer(stddev=0.1)\n",
    "        self.session=session\n",
    "        self.init_tf_variable()\n",
    "        \n",
    "        init_tf=tf.initialize_all_variables()\n",
    "        \n",
    "        self.session.run(init_tf)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def return_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def init_tf_variable(self):\n",
    "        #self.story_embedding_W=tf.Variable(self.initializer)\n",
    "        self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"story_embedding_W\")\n",
    "        #self.story_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"story_embedding_W\")\n",
    "        #self.story_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"story_embedding_b\")\n",
    "\n",
    "        self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=1.0/math.sqrt(float(len(self.vocab)))),name=\"query_embedding_W\")\n",
    "        #self.query_embedding_W=tf.Variable(tf.truncated_normal([len(self.vocab),self.memory_dim],stddev=0.1),name=\"query_embedding_W\")\n",
    "        #self.query_embedding_b=tf.Variable(tf.zeros([self.memory_dim]),name=\"query_embedding_b\")\n",
    "\n",
    "        self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=1.0/math.sqrt(float(self.memory_dim))),name=\"output_W\")\n",
    "        #self.output_W=tf.Variable(tf.truncated_normal([self.memory_dim,len(self.vocab)],stddev=0.1),name=\"output_W\")\n",
    "        #self.output_b=tf.Variable(tf.zeros([len(self.vocab)]),name=\"output_b\")\n",
    "\n",
    "        self.TA = tf.Variable(tf.truncated_normal([self.story_max,self.memory_dim],stddev=0.1))\n",
    "\n",
    "        self.story_batch=tf.placeholder(tf.float32,[self.batch_size,self.story_max,len(self.vocab)])\n",
    "        self.query_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "        self.answer_batch=tf.placeholder(tf.float32,[self.batch_size,1,len(self.vocab)])\n",
    "        self.memory_init_flag_batch=tf.placeholder(tf.int32,[self.batch_size])\n",
    "\n",
    "\n",
    "        self.cross_entropy_sum=tf.constant(0.0)\n",
    "        self.predict_list=tf.constant([[0.0]*len(self.vocab)])\n",
    "        self.cross_entropy_list=tf.constant([])\n",
    "        linear_start_cross_entropy_list=tf.constant([])\n",
    "        self.attention_list=[]\n",
    "        for idx in range(self.batch_size):\n",
    "            temp_attention_list=[]\n",
    "            #print(tf.constant(1))\n",
    "            #print()\n",
    "\n",
    "            #self.memory=tf.mul(self.memory,tf.sub(tf.constant(1.0),tf.cast(self.memory_init_flag_batch[idx],dtype=tf.float32)))\n",
    "            #self.memory_pointer=tf.mul(self.memory_pointer,tf.sub(tf.constant(1),self.memory_init_flag_batch[idx]))\n",
    "            #self.embedding_story=tf.matmul(self.story_batch[idx],self.story_embedding_W)+self.story_embedding_b\n",
    "            #embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)+self.query_embedding_b\n",
    "            #self.memory=tf.cond(tf.equal(self.memory_init_flag_batch[idx],tf.constant(1)),self.init_memory,self.return_memory)\n",
    "            self.embedding_story=tf.add(tf.matmul(self.story_batch[idx],self.story_embedding_W),self.TA) \n",
    "            embedding_query=tf.matmul(self.query_batch[idx],self.query_embedding_W)\n",
    "            #tf.scatter_update(self.memory,self.memory_pointer,self.embedding_story)\n",
    "\n",
    "            #self.memory=tf.concat(0,[self.memory,self.embedding_story])\n",
    "            \"\"\"\n",
    "            for elem in tf.split(0,self.embedding_story.get_shape()[0],self.embedding_story):\n",
    "                #print(self.memory)\n",
    "                self.memory=tf.concat(0,[self.memory,elem])\n",
    "                #self.memorytf.reshape(elem,[-1])\n",
    "                #self.memory_pointer=tf.add(self.memory_pointer,1)\n",
    "            \"\"\"\n",
    "            #self.memory=tf.concat(self.memory,self.embedding_story)\n",
    "            self.u=embedding_query\n",
    "            linear_start_u=self.u\n",
    "            for i in range(self.hop):\n",
    "\n",
    "                self.attention=tf.nn.softmax(tf.reshape(tf.reduce_sum(tf.mul(self.embedding_story,self.u),1),[1,-1]))\n",
    "                temp_attention_list.append(self.attention[0])\n",
    "                self.attended_embedding=tf.matmul(self.attention,self.embedding_story)\n",
    "                self.u=tf.add(self.u,self.attended_embedding)\n",
    "\n",
    "                linear_start_attention=tf.reshape(tf.reduce_sum(tf.mul(self.embedding_story,self.u),1),[1,-1])\n",
    "                linear_start_attended_embedding=tf.matmul(linear_start_attention,self.embedding_story)\n",
    "                linear_start_u=tf.add(linear_start_u,linear_start_attended_embedding)\n",
    "                \n",
    "\n",
    "            predict=tf.matmul(self.u,self.output_W)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(predict, tf.cast(self.answer_batch[idx], tf.float32), name=\"cross_entropy\")\n",
    "            self.cross_entropy_list=tf.concat(0,[self.cross_entropy_list,cross_entropy])\n",
    "            \n",
    "            linear_start_predict=tf.matmul(linear_start_u,self.output_W)\n",
    "            linear_start_cross_entropy=tf.nn.softmax_cross_entropy_with_logits(linear_start_predict, tf.cast(self.answer_batch[idx], tf.float32), name=\"cross_entropy\")\n",
    "            linear_start_cross_entropy_list=tf.concat(0,[linear_start_cross_entropy_list,linear_start_cross_entropy])\n",
    "            \n",
    "            self.predict_list=tf.concat(0,[self.predict_list,tf.nn.softmax(predict)])\n",
    "            self.attention_list.append(temp_attention_list)\n",
    "        self.predict_list=self.predict_list[1:]\n",
    "        self.cross_entropy_sum=tf.reduce_sum(self.cross_entropy_list)\n",
    "        self.linear_start_cross_entropy_sum=tf.reduce_sum(linear_start_cross_entropy_list)\n",
    "\n",
    "        story_embedding_W_hist=tf.histogram_summary(\"story_embedding_W\",self.story_embedding_W)\n",
    "        #story_embedding_b_hist=tf.histogram_summary(\"story_embedding_b\",self.story_embedding_b)\n",
    "        query_embedding_W_hist=tf.histogram_summary(\"query_embedding_W\",self.query_embedding_W)\n",
    "        #query_embedding_b_hist=tf.histogram_summary(\"query_embedding_b\",self.query_embedding_b)\n",
    "        output_W_hist=tf.histogram_summary(\"output_W\",self.output_W)\n",
    "        #output_b_hist=tf.histogram_summary(\"output_b\",self.output_b)\n",
    "        cross_entropy_sum_scalar = tf.scalar_summary(\"cross_entropy_sum\",self.cross_entropy_sum)\n",
    "        self.merged=tf.merge_all_summaries()\n",
    "        self.writer=tf.train.SummaryWriter(\"/home/asd36952/tfbd/memnet/Memorynet_our_data_hop_batch_linear_start\",self.session.graph)\n",
    "\n",
    "        self.train_step=tf.train.AdamOptimizer(learning_rate=1e-2).minimize(self.cross_entropy_sum)\n",
    "        self.linear_start_train_step=tf.train.AdamOptimizer(learning_rate=1e-2).minimize(self.linear_start_cross_entropy_sum)\n",
    "                #self.train_step=tf.contrib.layers.optimize_loss(loss=self.cross_entropy_sum,learning_rate=1e-2,optimizer='Adam',gradient_noise_scale=1e-4,global_step=tf.contrib.framework.get_global_step())\n",
    "                #grads_and_vars = tf.train.AdamOptimizer(learning_rate=1e-2).compute_gradients(self.cross_entropy_sum)\n",
    "                #grads_and_vars = [(tf.clip_by_norm(g, 40), v) for g,v in grads_and_vars]\n",
    "                #self.train_step = tf.train.AdamOptimizer(learning_rate=1e-2).apply_gradients(grads_and_vars)\n",
    "    \n",
    "    def batch_fit(self,train,epoch=100):\n",
    "        train_dict=dict()\n",
    "        for elem in train:\n",
    "            train_dict[elem[-1]]=elem[:-1]\n",
    "        train_data,valid_data=self.preprocessing(train)\n",
    "        num_train=np.array(train_data).shape[0]\n",
    "        num_valid=np.array(valid_data).shape[0]\n",
    "        \n",
    "        linear_start_flag=True\n",
    "        before_loss=np.inf\n",
    "        for ep in range(epoch):\n",
    "            np.random.shuffle(train_data)\n",
    "            for idx in range(int(num_train/self.batch_size)):\n",
    "                current_time=time.time()\n",
    "                if num_train-idx*self.batch_size<self.batch_size:\n",
    "                    break\n",
    "                batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                #print(np.array(batch_train)[:,0].tolist())\n",
    "                #print(np.array(np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist()).shape)\n",
    "                if linear_start_flag==True:\n",
    "                    summary,_=self.session.run([self.merged,self.linear_start_train_step],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                                  self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                                  self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                                  self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                else:\n",
    "                    summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                                                  self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                                  self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                                                  self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                self.writer.add_summary(summary,ep*num_train+idx)\n",
    "            if (ep+1)%10==0:\n",
    "                print(\"epoch:\",ep+1)\n",
    "                save_file=open(\"./memnet_out/Memorynet_our_data_hop_batch_linear_start_\"+str(self.memory_dim)+\"_\"+str(ep+1)+\".txt\",\"w\")\n",
    "                total_cross_ent=0\n",
    "                total=0\n",
    "                correct=0\n",
    "                for idx in range(int(num_train/self.batch_size)):\n",
    "                    if num_train-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    batch_train=train_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                    temp_cross_ent,predict_list,attention_list=self.session.run([self.cross_entropy_sum,self.predict_list,self.attention_list],feed_dict={self.story_batch:np.array(batch_train)[:,0].tolist(),\n",
    "                                              self.query_batch:np.array(np.array(batch_train)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.answer_batch:np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.memory_init_flag_batch:np.array(batch_train)[:,3].tolist()})\n",
    "                    total_cross_ent+=temp_cross_ent\n",
    "                    #print(predict_list)\n",
    "                    answer_list=np.array(np.array(batch_train)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                    for batch_idx in range(self.batch_size):\n",
    "                        total+=1\n",
    "                        if np.argmax(predict_list[batch_idx])==np.argmax(answer_list[batch_idx]):\n",
    "                            correct+=1\n",
    "                        for i in range(self.story_max):\n",
    "                            if max(batch_train[batch_idx][0][i])==0:\n",
    "                                break\n",
    "                            save_file.write(str(train_dict[batch_train[batch_idx][-1]][0][i])+\"\\t\")\n",
    "                            for j in range(self.hop):\n",
    "                                save_file.write(str(attention_list[batch_idx][j][i])+\"\\t\")\n",
    "                            save_file.write(\"\\n\")\n",
    "                        save_file.write(str(train_dict[batch_train[batch_idx][-1]][1])+\"\\n\")\n",
    "                        save_file.write(\"Our result: \"+str(self.vocab[np.argmax(predict_list[batch_idx])])+\"\\tAnswer: \"+str(train_dict[batch_train[batch_idx][-1]][2])+\"\\n\\n\")\n",
    "                        #print(batch_train[batch_idx][1])\n",
    "                        #print(self.from_bag_of_word_to_sentence(batch_train[batch_idx][1]))\n",
    "                        #print(self.from_bag_of_word_to_sentence(batch_train[batch_idx][2]))\n",
    "                        #print(attention_list[0])\n",
    "                        #print(attention_list[1])\n",
    "                        #print(\"*******************\")\n",
    "                save_file.close()\n",
    "                if total_cross_ent>=before_loss:\n",
    "                    linear_start_flag=False\n",
    "                else:\n",
    "                    before_loss=total_cross_ent\n",
    "                print(\"total loss:\",total_cross_ent)\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                print(\"#############################\")\n",
    "                total=0\n",
    "                correct=0\n",
    "                for idx in range(int(num_valid/self.batch_size)):\n",
    "                    if num_valid-idx*self.batch_size<self.batch_size:\n",
    "                        break\n",
    "                    batch_valid=valid_data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                    predict_list=self.session.run(self.predict_list,feed_dict={self.story_batch:np.array(batch_valid)[:,0].tolist(),\n",
    "                                              self.query_batch:np.array(np.array(batch_valid)[:,1].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.answer_batch:np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist(),\n",
    "                                              self.memory_init_flag_batch:np.array(batch_valid)[:,3].tolist()})\n",
    "                    #print(predict_list)\n",
    "                    answer_list=np.array(np.array(batch_valid)[:,2].tolist()).reshape(-1,1,len(self.vocab)).tolist()\n",
    "                    for idx in range(self.batch_size):\n",
    "                        total+=1\n",
    "                        if np.argmax(predict_list[idx])==np.argmax(answer_list[idx]):\n",
    "                            correct+=1\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                print(\"#############################\")\n",
    "                print(\"#############################\")\n",
    "    \n",
    "    def from_bag_of_word_to_sentence(self,bag_of_word):\n",
    "        return np.array(self.vocab)[np.where(bag_of_word)].tolist()\n",
    "    \n",
    "    def fit(self,train_data,epoch=2000):\n",
    "        train_data,valid_data=self.preprocessing(train_data)\n",
    "        print(\"Learning ...\")\n",
    "        for ep in range(epoch):\n",
    "            for story,query,answer,memory_init_flag in train_data:\n",
    "                #print(story)\n",
    "                summary,_=self.session.run([self.merged,self.train_step],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag})\n",
    "                self.writer.add_summary(summary,ep)\n",
    "            if (ep+1)%10==0:\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in train_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"train\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "                total=0\n",
    "                correct=0\n",
    "                for story,query,answer,memory_init_flag in valid_data:\n",
    "                    total+=1\n",
    "                    if self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],\n",
    "                                                                                       self.memory_init_flag:memory_init_flag}))]==self.vocab[np.argmax(answer)]:\n",
    "                        correct+=1\n",
    "                print(\"valid\")\n",
    "                print(total)\n",
    "                print(correct)\n",
    "                print(correct/total)\n",
    "        for story,query,answer,memory_init_flag in train_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        print(\"#############################\")\n",
    "        for story,query,answer,memory_init_flag in valid_data:\n",
    "            print(self.vocab[np.argmax(self.session.run([self.predict],feed_dict={self.story:story,self.query:[query],self.answer:[answer],self.memory_init_flag:memory_init_flag}))])\n",
    "            print(self.vocab[np.argmax(answer)])\n",
    "            print(\"#############################\")\n",
    "        \n",
    "        \n",
    "        #print(self.session.run([self.embedding_story,self.emdedding_query],feed_dict={self.story:story,self.query:query,self.answer:answer,self.memory_init_flag:memory_init_flag}))\n",
    "    \n",
    "    def preprocessing(self,train_data,validation_cut=0.8):\n",
    "        preprocessed_data=[]\n",
    "        for story,query,answer,memory_init_flag,idx in train_data:\n",
    "            story_list=[]\n",
    "            for sentence in story:\n",
    "                sentence_vector=[0]*len(self.vocab)\n",
    "                for word in sentence:\n",
    "                    sentence_vector[self.vocab.index(word)]=1\n",
    "                story_list.append(sentence_vector)\n",
    "            for _ in range(self.story_max-len(story_list)):\n",
    "                story_list.append([0]*len(self.vocab))\n",
    "            query_vector=[0]*len(self.vocab)\n",
    "            for word in query:\n",
    "                query_vector[self.vocab.index(word)]=1\n",
    "            answer_vector=[0]*len(self.vocab)\n",
    "            for word in answer:\n",
    "                answer_vector[self.vocab.index(word)]=1\n",
    "            preprocessed_data.append([story_list,query_vector,answer_vector,memory_init_flag,idx])\n",
    "        #print(preprocessed_data)\n",
    "        return preprocessed_data[:int(len(preprocessed_data)*validation_cut)],preprocessed_data[int(len(preprocessed_data)*validation_cut):]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_memory(self):\n",
    "        return tf.constant([[0.0]*self.memory_dim],dtype=tf.float32)\n",
    "        #self.memory=tf.Variable(initial_value=[[0.0]*self.memory_dim]*self.memory_max,trainable=False)\n",
    "        #self.memory=[[0.0]*self.memory_dim]*self.memory_max\n",
    "        #self.memory_pointer=tf.constant(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.empty([0,3])\n",
    "b=np.array([1,2,3])\n",
    "c=np.array([1,2,3,4])\n",
    "d=np.array([1,2,3,4,5])\n",
    "print(np.append(a,np.array([[b,c,d]]),0))\n",
    "#print(np.append(np.append(a,b,0),b,0))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babi_example=[[[\"Mary moved to the bathroom.\",\"John went to the hallway.\"],\"Where is Mary?\", \"bathroom\",0],\n",
    "              [[\"Daniel went back to the hallway.\",\"Sandra moved to the garden.\"],\"Where is Daniel?\",\"hallway\",1],\n",
    "              [[\"John moved to the office\",\"Sandra journeyed to the bathroom.\"],\"Where is Daniel?\",\"hallway\",1],\n",
    "              [[\"Mary moved to the hallway.\",\"Daniel travelled to the office.\"],\"Where is Daniel?\",\"office\",1],\n",
    "              [[\"John went back to the garden.\",\"John moved to the bedroom.\"],\"Where is Sandra?\",\"bathroom\",1],\n",
    "              [[\"Sandra travelled to the office.\",\"Sandra went to the bathroom.\"],\"Where is Sandra?\", \"bathroom\",0],\n",
    "              [[\"Mary went to the bedroom.\",\"Daniel moved to the hallway.\"],\"Where is Sandra?\", \"bathroom\",1],\n",
    "              [[\"John went to the garden.\",\"John travelled to the office.\"],\"Where is Sandra?\",  \"bathroom\",1],    \n",
    "              [[\"Daniel journeyed to the bedroom.\",\"Daniel travelled to the hallway.\"], \"Where is John?\", \"office\",1],\n",
    "              [[\"John went to the bedroom.\",\"John travelled to the office.\"],\"Where is Daniel?\", \"hallway\",1]]\n",
    "preprocessed=[]\n",
    "vocab=[]\n",
    "for story,query,answer,memory_init_flag in babi_example:\n",
    "    sentence_list=[]\n",
    "    for sentence in story:\n",
    "        sentence_list.append(nltk.word_tokenize(sentence))\n",
    "    for sentence in sentence_list:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    query_list=nltk.word_tokenize(query)\n",
    "    for word in query_list:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "    answer_list=nltk.word_tokenize(answer)\n",
    "    for word in answer_list:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "    preprocessed.append([sentence_list,query_list,answer_list,memory_init_flag])\n",
    "print(preprocessed[0])\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    a=MN(vocab,10,30,2,sess)\n",
    "    a.fit(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.41786873+0.61164886\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([[[[0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 0], [[[0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "d=[[[tf.constant(0),tf.constant(0)],[tf.constant(0),tf.constant(0)]]]*2\n",
    "with tf.device('/gpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.mul(a, b)\n",
    "  d[0]=c\n",
    "  e = tf.mul(d,tf.constant(3.0))\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "d=[[tf.constant(0.0)]*3]*5\n",
    "with tf.device('/gpu:0'):\n",
    "  a = tf.constant([[[1.0, 2.0, 3.0,4.0],[5.0,6.0,7.0,8.0],[8.0,9.0,10.0,11.0]]], name='a')\n",
    "  b = tf.constant([[3.0, 4.0, 5.0]], name='b')\n",
    "  #d[0]=tf.cast(a,tf.float32)\n",
    "  f=tf.matmul(b,a)\n",
    "  #e = a*[tf.constant(3.0),tf.constant(4.0),tf.constant(5.0)]\n",
    "  e = tf.matmul(([[tf.constant(3.0),tf.constant(4.0),tf.constant(5.0)]]),a)\n",
    "  #g=tf.reduce_sum(a*b,1)\n",
    "  h=b+b\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(e))\n",
    "print(sess.run(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant(5.0, shape=[5, 6])\n",
    "w = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "xw = tf.mul(x, w)\n",
    "max_in_rows = tf.reduce_max(xw, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(xw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
