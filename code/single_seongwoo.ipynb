{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Task: 2\n",
      "Longest sentence length 1496\n",
      "Longest story length 169\n",
      "Average story length 18\n",
      "[[12257  6738     0 ...,     0     0     0]\n",
      " [12257   678  4995 ...,     0     0     0]\n",
      " [ 7441  8140 11978 ...,     0     0     0]\n",
      " ..., \n",
      " [    0     0     0 ...,     0     0     0]\n",
      " [    0     0     0 ...,     0     0     0]\n",
      " [    0     0     0 ...,     0     0     0]]\n",
      "Training set shape (263, 169, 1496)\n",
      "Training Size 263\n",
      "Validation Size 30\n",
      "Testing Size 293\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example running MemN2N on a single bAbI task.\n",
    "Download tasks from facebook.ai/babi \"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from data_utils import load_task, vectorize_data\n",
    "from sklearn import cross_validation, metrics\n",
    "from memn2n import MemN2N\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate for Adam Optimizer.\")\n",
    "tf.flags.DEFINE_float(\"epsilon\", 1e-8, \"Epsilon value for Adam Optimizer.\")\n",
    "tf.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\n",
    "tf.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 200, \"Number of epochs to train for.\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 4, \"Embedding size for embedding matrices.\")\n",
    "tf.flags.DEFINE_integer(\"memory_size\", 500, \"Maximum size of memory.\")\n",
    "tf.flags.DEFINE_integer(\"task_id\", 1, \"bAbI task id, 1 <= id <= 20\")\n",
    "tf.flags.DEFINE_integer(\"random_state\", None, \"Random state.\")\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"data/tasks_1-20_v1-2/en/\", \"Directory containing bAbI tasks\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "FLAGS.task_id=2\n",
    "print(\"Started Task:\", FLAGS.task_id)\n",
    "\n",
    "# task data\n",
    "#train, test = load_task(FLAGS.data_dir, FLAGS.task_id)\n",
    "import util\n",
    "queries=util.load_our_queries_morp()\n",
    "train=[]\n",
    "for idx in range(len(queries)):\n",
    "    train.append([util.load_stories_with_our_query(queries[idx]),queries[idx][0],queries[idx][1]]) \n",
    "test = train\n",
    "data = train + test\n",
    "#print(type(train[0]))\n",
    "#print(train[0])\n",
    "#exit()\n",
    "vocab=[]\n",
    "\n",
    "for stories,query,answer in train:\n",
    "    for sent in stories:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    for word in query:\n",
    "        vocab.append(word)\n",
    "    for word in answer:\n",
    "        vocab.append(word)\n",
    "vocab=list(set(vocab))\n",
    "\n",
    "#vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n",
    "sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "query_size = max(map(len, (q for _, q, _ in data)))\n",
    "memory_size = min(FLAGS.memory_size, max_story_size)\n",
    "vocab_size = len(word_idx) + 1 # +1 for nil word\n",
    "sentence_size = max(query_size, sentence_size) # for the position\n",
    "\n",
    "print(\"Longest sentence length\", sentence_size)\n",
    "print(\"Longest story length\", max_story_size)\n",
    "print(\"Average story length\", mean_story_size)\n",
    "\n",
    "# train/validation/test sets\n",
    "S, Q, A = vectorize_data(train, word_idx, sentence_size, memory_size)\n",
    "trainS, valS, trainQ, valQ, trainA, valA = cross_validation.train_test_split(S, Q, A, test_size=.1, random_state=FLAGS.random_state)\n",
    "testS, testQ, testA = vectorize_data(test, word_idx, sentence_size, memory_size)\n",
    "\n",
    "print(testS[0])\n",
    "\n",
    "print(\"Training set shape\", trainS.shape)\n",
    "\n",
    "# params\n",
    "n_train = trainS.shape[0]\n",
    "n_test = testS.shape[0]\n",
    "n_val = valS.shape[0]\n",
    "\n",
    "print(\"Training Size\", n_train)\n",
    "print(\"Validation Size\", n_val)\n",
    "print(\"Testing Size\", n_test)\n",
    "\n",
    "train_labels = np.argmax(trainA, axis=1)\n",
    "test_labels = np.argmax(testA, axis=1)\n",
    "val_labels = np.argmax(valA, axis=1)\n",
    "\n",
    "tf.set_random_seed(FLAGS.random_state)\n",
    "batch_size = FLAGS.batch_size\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate, epsilon=FLAGS.epsilon)\n",
    "\n",
    "batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "batches = [(start, end) for start, end in batches]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = MemN2N(batch_size, vocab_size, sentence_size, memory_size, FLAGS.embedding_size, session=sess,\n",
    "                   hops=FLAGS.hops, max_grad_norm=FLAGS.max_grad_norm, optimizer=optimizer)\n",
    "    for t in range(1, FLAGS.epochs+1):\n",
    "        np.random.shuffle(batches)\n",
    "        total_cost = 0.0\n",
    "        for start, end in batches:\n",
    "            s = trainS[start:end]\n",
    "            q = trainQ[start:end]\n",
    "            a = trainA[start:end]\n",
    "            cost_t = model.batch_fit(s, q, a)\n",
    "            total_cost += cost_t\n",
    "\n",
    "        if t % FLAGS.evaluation_interval == 0:\n",
    "            train_preds = []\n",
    "            for start in range(0, n_train, batch_size):\n",
    "                end = start + batch_size\n",
    "                s = trainS[start:end]\n",
    "                q = trainQ[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                train_preds += list(pred)\n",
    "\n",
    "            val_preds = model.predict(valS, valQ)\n",
    "            train_acc = metrics.accuracy_score(np.array(train_preds), train_labels)\n",
    "            val_acc = metrics.accuracy_score(val_preds, val_labels)\n",
    "\n",
    "            print('-----------------------')\n",
    "            print('Epoch', t)\n",
    "            print('Total Cost:', total_cost)\n",
    "            print('Training Accuracy:', train_acc)\n",
    "            print('Validation Accuracy:', val_acc)\n",
    "            print('-----------------------')\n",
    "\n",
    "    test_preds = model.predict(testS, testQ)\n",
    "    test_acc = metrics.accuracy_score(test_preds, test_labels)\n",
    "    print(\"Testing Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
